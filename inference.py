import torch
import cv2
import numpy as np
from pathlib import Path
import argparse
import onnxruntime as ort

from config.config import cfg
from data.transforms import get_transforms
from models.resnet_model import ResNetCanClassifier


class CanRotationPredictor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —É–≥–ª–∞ –ø–æ–≤–æ—Ä–æ—Ç–∞ –±–∞–Ω–∫–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π grayscale –∏ ONNX"""

    def __init__(self, model_path):
        self.device = cfg.DEVICE
        self.transform = get_transforms('val')
        self.model_path = Path(model_path)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é —Ñ–∞–π–ª–∞
        if self.model_path.suffix.lower() == '.onnx':
            self.model_type = 'onnx'
            self._init_onnx_model()
        else:
            self.model_type = 'pytorch'
            self._init_pytorch_model()

        print(f"‚úÖ Model loaded from {model_path}")
        print(f"‚úÖ Model type: {self.model_type.upper()}")
        print(f"‚úÖ Inference mode: {'Grayscale' if cfg.GRAYSCALE else 'RGB'}")

    def _init_pytorch_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PyTorch –º–æ–¥–µ–ª–∏"""
        self.model = ResNetCanClassifier(num_classes=cfg.NUM_CLASSES)
        checkpoint = torch.load(self.model_path, map_location='cpu')

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É checkpoint
        if 'model_state_dict' in checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        else:
            # –ï—Å–ª–∏ checkpoint —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ä–∞–∑—É state_dict –º–æ–¥–µ–ª–∏
            self.model.load_state_dict(checkpoint)

        self.model.to(self.device)
        self.model.eval()

        if 'best_accuracy' in checkpoint:
            print(f"‚úÖ Best accuracy in training: {checkpoint['best_accuracy']}%")

    def _init_onnx_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ONNX –º–æ–¥–µ–ª–∏"""
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é ONNX Runtime
        providers = ['CPUExecutionProvider']
        if self.device == 'cuda' and 'CUDAExecutionProvider' in ort.get_available_providers():
            providers = ['CUDAExecutionProvider'] + providers

        self.session = ort.InferenceSession(
            str(self.model_path),
            providers=providers
        )

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Ö–æ–¥–µ –º–æ–¥–µ–ª–∏
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name

        print(f"‚úÖ ONNX Input: {self.input_name}, Output: {self.output_name}")
        print(f"‚úÖ ONNX Providers: {providers}")

    def predict_from_tensor(self, image_tensor):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏–∑ –≥–æ—Ç–æ–≤–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ (–¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏)"""
        if self.model_type == 'pytorch':
            return self._predict_pytorch(image_tensor)
        else:
            return self._predict_onnx(image_tensor)

    def _predict_pytorch(self, image_tensor):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyTorch –º–æ–¥–µ–ª–∏"""
        image_tensor = image_tensor.to(self.device)

        with torch.no_grad():
            outputs = self.model(image_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            predicted_class = torch.argmax(outputs, dim=1)
            confidence = torch.max(probabilities, dim=1)[0].item()

        return {
            'angle': predicted_class.item(),
            'confidence': confidence,
            'probabilities': probabilities.cpu().numpy()[0],
            'raw_output': outputs.cpu().numpy()[0]
        }

    def _predict_onnx(self, image_tensor):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ONNX –º–æ–¥–µ–ª–∏"""
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–Ω–∑–æ—Ä –≤ numpy array
        input_data = image_tensor.numpy().astype(np.float32)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
        outputs = self.session.run([self.output_name], {self.input_name: input_data})
        outputs_tensor = torch.from_numpy(outputs[0])

        # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –∏ –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        probabilities = torch.softmax(outputs_tensor, dim=1)
        predicted_class = torch.argmax(outputs_tensor, dim=1)
        confidence = torch.max(probabilities, dim=1)[0].item()

        return {
            'angle': predicted_class.item(),
            'confidence': confidence,
            'probabilities': probabilities.numpy()[0],
            'raw_output': outputs[0][0]
        }

    def predict(self, image_path):
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —É–≥–æ–ª –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = cv2.imread(str(image_path))
        if image is None:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ü–≤–µ—Ç–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
        if cfg.GRAYSCALE:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            image = np.expand_dims(image, axis=-1)  # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–Ω–∞–ª—å–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ
        else:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º—ã
        transformed = self.transform(image=image)
        image_tensor = transformed['image'].unsqueeze(0)

        return self.predict_from_tensor(image_tensor)

    def predict_batch(self, image_paths):
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —É–≥–ª—ã –¥–ª—è –±–∞—Ç—á–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        results = []
        for image_path in image_paths:
            try:
                result = self.predict(image_path)
                result['file_path'] = str(image_path)
                results.append(result)
            except Exception as e:
                print(f"‚ùå Error processing {image_path}: {e}")
        return results


def main():
    parser = argparse.ArgumentParser(description='Predict soda can rotation angle')
    parser.add_argument('--model', type=str, required=True,
                        help='Path to trained model checkpoint (.pth or .onnx)')
    parser.add_argument('--image', type=str, required=True,
                        help='Path to image or directory for prediction')
    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏
    if not Path(args.model).exists():
        print(f"‚ùå Error: Model file {args.model} not found!")
        return

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º predictor
    predictor = CanRotationPredictor(args.model)
    input_path = Path(args.image)

    if input_path.is_file():
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        result = predictor.predict(input_path)
        print(f"üìä Image: {input_path}")
        print(f"üéØ Predicted angle: {result['angle']}¬∞")
        print(f"üìà Confidence: {result['confidence']:.4f}")

    elif input_path.is_dir():
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø–∞–ø–∫–µ
        image_extensions = ['*.jpg', '*.png', '*.jpeg', '*.bmp']
        image_paths = []
        for ext in image_extensions:
            image_paths.extend(input_path.glob(ext))
            image_paths.extend(input_path.glob(ext.upper()))

        if not image_paths:
            print(f"‚ùå No images found in {input_path}")
            return

        results = predictor.predict_batch(image_paths)

        print(f"üìä Processed {len(results)} images:")
        for result in results:
            print(f"  üìÅ {Path(result['file_path']).name}: "
                  f"{result['angle']}¬∞ (conf: {result['confidence']:.4f})")

    else:
        print(f"‚ùå Error: {args.image} not found!")


if __name__ == '__main__':
    main()